---
title: 'Bank Marketing Data Analysis - Project Report'
author: "STAT 420, Summer 2020, Bhushan Bhathani, Matthew Leung, Vijayakumar Sitha Mohan,"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

<style type="text/css">
.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
  color: blue;
}
</style>

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r message=FALSE, warning=FALSE}
#install.packages("e1071")
library(knitr)      # web widget
library(tidyverse)  # data manipulation
library(data.table) # fast file reading
library(caret)      # rocr analysis
library(ROCR)       # rocr analysis
library(kableExtra) # nice table html formating 
library(gridExtra)  # arranging ggplot in grid
library(rpart)      # decision tree
library(rpart.plot) # decision tree plotting
library(caTools)    # split
library(lmtest)
library(randomForest)
library(ada)
library(boot)
library(e1071)      # caret depends on this
library(ada)
library(ROSE)
library(DMwR)
library(latticeExtra)
library(pROC)
```

## Introduction
<span style="color:Blue">

Our team selected a dataset of direct banking marketing of a Portuguese bank. The goal is to predict if the client subscribe to the bank term deposit. The dataset has client personal attributes, marketing campaign attributes, social and economic context attributes. The response variable (y) is binomial with value either 1 (subscribe) or 0 (not subscribe).

Our team is interested in various classification technique performance. We selected this dataset because it is a clean dataset for classification. It has been top ten most popular from the data repository that we download from. We can focus on analyzing classification techniques rather then cleaning data. The URL to the dataset is http://archive.ics.uci.edu/ml/datasets/Bank+Marketing.


We picked the following three classification techniques for this project:

- Logistic Regression (It is covered in this course STAT420)

- Random Forest (Very popular technique in Ensemble learning with a number of weak classifier based on decision tree)

- Adaptive Boosting (More sophisticated technique in Ensemble learning with a number of weak classifier based on decision tree)
</span>

## Methods

```{r echo=FALSE}
bank_df1 = read.csv("data/bank-additional.csv")
```

### Understanding Data

- The Bank-Marketing dataset is downloaded from UCI Machine Learning Repository and the same is available at http://archive.ics.uci.edu/ml/datasets/Bank+Marketing.
- There were 4 datasets in it from which bank-additional.csv is used that has subset of 4,119 observations out of all available observations of 41,188 in the full dataset.
- 20 inputs ordered by date (from May 2008 to November 2010). There are 20 input variables and 1 output
variable (desired target). 
- The dataset has  customer data, socio-economic data, telemarketing data and some other data. Some attributes are numerical, and some are categorical.

There are 4 categories of data in the dataset.

    + Customer's demographic information   - age,job,marital,education,default,housing,loan
    + Customer's financial data - default, hosing loan, personal loan
    + Campaign and Marketing related data - contact,month,day_of_week,duration,campaign,pdays,previous,poutcome
    + Overall market and economic indicators - emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed

**Attribute Information:**

```{r echo=FALSE, message=FALSE, warning=FALSE}
columns = read.csv("data/col_names.csv")
kable(columns) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Output variable (desired target) is **Term Deposit** which is categorical binary variable.

#### Data Validation

**Checking for Missing value**
```{r}
any(is.na(bank_df1))
```

- There are `r sum(is.na(bank_df1))` rows with missing (NA) value in any of the columns.

#### Data Curation  (cleaning , missing data, duplicate)

```{r}
paste("There are ", sum(duplicated(bank_df1)), "Duplicate Row(s)")
```

**Remove Duplicates**
```{r}
bank_df = bank_df1 %>% distinct
```

**Recode categorical attributes as factor variables**

```{r}
bank_df$y = ifelse(bank_df$y=='yes',1,0)
bank_df$y = as.factor(bank_df$y)
bank_df$job = as.factor(bank_df$job)
bank_df$education = as.factor(bank_df$education)
bank_df$marital = as.factor(bank_df$marital)
bank_df$default = as.factor(bank_df$default)
bank_df$housing = as.factor(bank_df$housing)
bank_df$loan = as.factor(bank_df$loan)
bank_df$contact = as.factor(bank_df$contact)
bank_df$month = as.factor(bank_df$month)
bank_df$poutcome = as.factor(bank_df$poutcome)
bank_df$day_of_week = as.factor(bank_df$day_of_week)
bank_df$default = as.factor(bank_df$default)
```

**Verify Levels of Categorical attributes**

```{r}
levels(bank_df$job)
levels(bank_df$marital)
levels(bank_df$education)
levels(bank_df$default)
levels(bank_df$housing)
levels(bank_df$loan)
levels(bank_df$contact)
levels(bank_df$month)
levels(bank_df$poutcome)
```

#### Data Analysis


<span style="color:Blue"> 
Let's see the levels and frequency of these data at higher level in histogram to have a sense of data distributed.

</span>

**Attribute wise Frequency Distributions**

```{r}
par(mfrow=c(2,2))
for(i in 1:21){
  barplot(prop.table(table(bank_df[,i])) , 
          xlab=names(bank_df[i]), ylab= "Frequency (%)" , col = rainbow(3), horiz = FALSE)
}
```



```{r echo=FALSE}
#pairs_df = subset(bank_df, select = c(1, 10, 11, 13:17))
#pairs(pairs_df, pch = 19, gap = .25, xaxt = "n", yaxt = "n", col = c("#0080FF", "#F3953E")[bank_df$y], label.pos = .5, oma = c(1, 1, 1, 1))

```

<span style="color:Blue"> 
Let's analyze the dataset for any linearity between predictors and y using boxplot and pairs.
</span>



```{r}
pairs(bank_df[c(1:4,21)], pch = 19, gap = .25, xaxt = "n", yaxt = "n"
      , col = c("#0080FF", "#F3953E")[bank_df$y]
      , label.pos = .5, oma = c(1, 1, 1, 1))
```

<span style="color:Blue">
- pairs() function, which plots all possible scatterplots between pairs of variables in the dataset.

- There are no obvious colinearity between predictors `age`,`job`,`education` and `marital`.
</span>


```{r}
par(mfrow=c(2,2))

#--- This is duplicate code. already we factorized this attributes.

bank_df$jobclass = as.numeric(factor(bank_df$job,levels=c("unknown","unemployed","housemaid", "student","retired","technician","blue-collar","admin.","services","self-employed","management","entrepreneur"),ordered=TRUE))

bank_df$educationNum = as.numeric(factor(bank_df$education,levels=c("unknown","illiterate","basic.4y","basic.6y","basic.9y","high.school","professional.course", "university.degree"),ordered=TRUE))

#---

boxplot(age~y,data=bank_df,main="age vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(jobclass~y,data=bank_df,main="Job vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(marital)~y,data=bank_df,main="Marital status vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(educationNum~y,data=bank_df,main="Education vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```

<span style="color:Blue"> 

Based on the above bar plots of individual predictors against `Term Deposit`, 

- It is evident that the Customers in the age group between 30 and 50 years subscribes to the term deposit than other age groups in the dataset.

- Customers with an education higher than high school takes the loan and subscribes to the term deposit vs not.

- Customers who are married subscribes to the term deposit more compared to others.

- Customers with job title as technician, blue-color, admin and work in services sector subscribes to the term deposit than others.

- So over all, It seems that age have some impact on term deposit subscription but need to see which exact range between 30-50 in detail. Rest parameters does not have any significant effect based on plot.

</span>


```{r}
pairs(bank_df[c(5:7,21)])
```


```{r}
par(mfrow=c(1,3))
boxplot(as.numeric(default)~y,data=bank_df,main="Default vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(housing)~y,data=bank_df,main="Housing Loan vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(loan)~y,data=bank_df,main="Personal loan vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```

<span style="color:Blue">
- Customers with the housing loan and personal loan has no impact on term deposit subscription. 
- Customers already in default certainly did not subscribed to the term deposit. So default can be a very good predictor.
</span>


```{r}
pairs(bank_df[c(8:15,21)])
par(mfrow=c(2,2))

levels(bank_df$contact)
boxplot(as.numeric(contact)~y,data=bank_df,main="Contact Type vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(month)~y,data=bank_df,main="last contact month of year vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(day_of_week)~y,data=bank_df,main=" last contact day of the week vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(duration~y,data=bank_df,main="last contact duration, in seconds vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```

<span style="color:Blue">
Based on plots, Customer with contact type cellular has more probability of subscribing for term deposit than telephone. People with recent contact made has more chance of subscribing for term deposit. Last contact day of the week does not have much ipact.Contact duraction has some impact on the term deposit subscription as well.
</span>


```{r}
par(mfrow=c(2,2))
boxplot(campaign~y,data=bank_df,main="# of contacts in this campaign vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(pdays~y,data=bank_df,main="number of days that passed by after the client was last contacted vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(previous~y,data=bank_df,main="# of contacts performed before this campaign vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(poutcome)~y,data=bank_df,main=" Previous Campain outcome vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```

<span style="color:Blue">Based on above plot, # of contact performed has some impact on Term deposit output else no other factor has significant impact.surprisingly # of contacts in this campaign decreases the probability to subscribe for term deposit.</span>

```{r}
pairs(bank_df[c(16:20,21)])
par(mfrow=c(3,2))
boxplot(emp.var.rate~y,data=bank_df,main="employment variation rate  vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(cons.price.idx~y,data=bank_df,main="consumer price index vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(cons.conf.idx~y,data=bank_df,main="consumer confidence index  vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(euribor3m~y,data=bank_df,main="euribor 3 month rate  vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(nr.employed~y,data=bank_df,main="number of employees vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```

<span style="color:Blue"> Based on plots all the market indicators has some impact on term deposit decision.</span>





#### Class Imbalance Problem analysis

- We say that the dataset have the class imbalance problem, where the main class of interest
is rare. That is, the data set distribution reflects a significant majority of the
negative class and a minority positive class. Below table investigation shows that. we have `88.8%` negative classes and `11.2%` positive classes.

```{r}
prop.table(table(bank_df$y))
```

- we need to address this problem otherwise our model won't able to predict correctly. Most of the traditional classifiers assume dataset have balanced class distribution.

- There are techniques available to overcome this class imbalance problem. Lets consider few of those techniques are:
    + Oversampling: Re-sampling of data from positive class
    
    + Under-sampling: Randomly eliminate tuples from negative class
    
    + Threshold-moving: Move the decision threshold, so that the rare class tuples are easier to classify, and hence, less chance of costly false negative errors.
    
- We have used two library packages `DMwR` and `ROSE` which has `SMOTE` and `ROSE` functions

    + SMOTE: The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset.
    
    + ROSE: ROSE (Random Over-Sampling Examples) aids the task of binary classification in the presence of rare classes. It produces a synthetic, possibly balanced, sample of data simulated according to a smoothed-bootstrap approach.
    
#### Train/Test data split

- First split the dataset into train and test split using `createDataPartition` from `caret` package. The below r code splits the dataset with 70:30 ratio.

```{r}
split<-createDataPartition(bank_df$y,p=0.7,list = FALSE)
train<-bank_df[split,]
test<-bank_df[-split,]
```

- Lets verify the class distribution in Training and Testing dataset. We see that our training and test dataset have same distribution of negative and positive classes.

```{r}
prop.table(table(train$y))
prop.table(table(test$y))
```

- Lets apply the class imbalances improvement techniques which we discussed above and verify the results.

```{r}
smote_train <- SMOTE(y ~ ., data = train)                         
rose_train <- ROSE(y ~ ., data = train)$data                   
```


```{r}
prop.table(table(smote_train$y))
prop.table(table(rose_train$y))
```


## Model Builing
### Model 1: Logistic Regression
####  model using all predictors



<span style="color:Blue"> As We have seen in boxplot comparison, age, default, contact type, last contact time, duration, # of contacts and all market indicators has some impact on terms deposit. Let's create a additive model using these parameters, full additive model and run AIC and BIC search to identify the model it comes up with.</span>



```{r}
bank_lr_plotobs = glm(y~age+default+campaign+previous+emp.var.rate+cons.price.idx+cons.conf.idx+euribor3m+nr.employed, data = train, family = "binomial")
bank_lr = glm(y ~ ., data = train, family = "binomial")
summary(bank_lr)
```

<span style="color:Blue"> Going through AIC and BIC backward search to find out the best model.</span>

```{r warning=FALSE}
n=length(resid(bank_lr))
bank_lr_aic = step(bank_lr, direction = "backward", trace = 0)
bank_lr_bic = step(bank_lr, direction = "backward", trace = 0,k=log(n))
bank_lr_aic
bank_lr_bic
```
<span style="color:Blue"> BIC comes with least number of parameters. Let's do the Anova LRT test to find the preferred model. </span>

```{r warning=FALSE}

result = data.frame(
        NullHypoThesis = c(ifelse(anova(bank_lr_bic,bank_lr,test="LRT")[2,5]<0.05,"Rejected","Fail To Reject"),
           ifelse(anova(bank_lr_aic,bank_lr,test="LRT")[2,5]<0.05,"Rejected","Fail To Reject"),
           ifelse(anova(bank_lr_bic,bank_lr_aic,test="LRT")[2,5]<0.05,"Rejected","Fail To Reject"),
           ifelse(anova(bank_lr_plotobs,bank_lr,test="LRT")[2,5]<0.05,"Rejected","Fail To Reject"),
           ifelse(anova(bank_lr_plotobs,bank_lr_aic,test="LRT")[2,5]<0.05,"Rejected","Fail To Reject")),
          PValue = c((anova(bank_lr_bic,bank_lr,test="LRT")[2,5]),
           (anova(bank_lr_aic,bank_lr,test="LRT")[2,5]),
           (anova(bank_lr_bic,bank_lr_aic,test="LRT")[2,5]),
           format(anova(bank_lr_plotobs,bank_lr,test="LRT")[2,5], nsmall = 5),
           format(anova(bank_lr_plotobs,bank_lr_aic,test="LRT")[2,5], nsmall = 5)),
row.names = c("BIC vs Additive","AIC vs Additive ","BIC vs AIC","PlotObs vs Full Additive","PlotObs vs AIC")
)



knitr::kable(result,format="markdown")

```

<span style="color:Blue">Based on Anova LRT test above, only AIC model is the preferable model among all model.</span>

<span style="color:Blue">Let's compare Null deviance, Residual Deviance, AIC and 5 fold misclassification rate to see which is the better model. </span>


```{r warning=FALSE}

mcRate_bank_lr = cv.glm(train,bank_lr_aic,K=5)$delta[1] # TO DO: change model
mcRate_bank_lr_aic = cv.glm(train,bank_lr_aic,K=5)$delta[1]
mcRate_bank_lr_bic = cv.glm(train,bank_lr_bic,K=5)$delta[1]
mcRate_bank_lr_plotobs = cv.glm(train,bank_lr_bic,K=5)$delta[1]#TO DO: Change Model



result = data.frame(
  Residual_Deviance=c(bank_lr$deviance,bank_lr_aic$deviance,bank_lr_bic$deviance,bank_lr_plotobs$deviance),
  Residual_DF=c(bank_lr$df.residual,bank_lr_aic$df.residual,bank_lr_bic$df.residual,bank_lr_plotobs$df.residual),
  Null_Deviance=c(bank_lr$null.deviance,bank_lr_aic$null.deviance,bank_lr_bic$null.deviance,bank_lr_plotobs$null.deviance),
  Null_DF=c(bank_lr$df.null,bank_lr_aic$df.null,bank_lr_bic$df.null,bank_lr_plotobs$df.null),
  AIC=c(AIC(bank_lr),AIC(bank_lr_aic),AIC(bank_lr_bic),AIC(bank_lr_plotobs)),
  MisclassificationRate=c(mcRate_bank_lr,mcRate_bank_lr_aic,mcRate_bank_lr_bic,mcRate_bank_lr_plotobs),
  row.names = c("Full Model","Model Selected with AIC","Model Selected with BIC","Model built based on boxplot observation")
)


knitr::kable(result,format="markdown")

```
<span style="color:Blue">  Based on the table above, AIC model has the lowest Residual Deviance after Full model but degree of freedom is more closer to Null Deviance degree of freedom than Full Model. AIC of the AIC model and misclassification rate is also least. Can we improve it further by applying the transformation or interaction on AIC model? Let's check.</span>

```{r warning=FALSE}

bank_lr_aic_twoway = glm(y ~ (job + education + default + contact + month + 
    day_of_week + duration + campaign + pdays + poutcome + emp.var.rate + 
    cons.price.idx + cons.conf.idx + euribor3m + nr.employed+ job:education+ month:day_of_week), 
    family = binomial(link="logit"), data = train)


bank_lr_aic_transform = glm(y ~ (job + education + default + contact + month + 
    day_of_week + duration + campaign + pdays + poutcome + emp.var.rate + 
    cons.price.idx + cons.conf.idx + euribor3m + log(nr.employed)), 
    family = binomial(link="logit"), data = train)

anova(bank_lr_aic,bank_lr_aic_twoway,test="LRT")

anova(bank_lr_aic,bank_lr_aic_transform,test="LRT")

mcRate_bank_lr_aic_twoway = cv.glm(train,bank_lr_aic,K=5)$delta[1] #  TO DO change model
mcRate_bank_lr_aic_transform = cv.glm(train,bank_lr_aic,K=5)$delta[1] # TO DO Change model

result = data.frame(
  Residual_Deviance=c(bank_lr$deviance,bank_lr_aic$deviance,bank_lr_bic$deviance,bank_lr_plotobs$deviance,bank_lr_aic$deviance,bank_lr_aic_twoway$deviance),
  Residual_DF=c(bank_lr$df.residual,bank_lr_aic$df.residual,bank_lr_bic$df.residual,bank_lr_plotobs$df.residual,bank_lr_aic$df.residual,bank_lr_aic_twoway$df.residual),
  Null_Deviance=c(bank_lr$null.deviance,bank_lr_aic$null.deviance,bank_lr_bic$null.deviance,bank_lr_plotobs$null.deviance,bank_lr_aic$null.deviance,bank_lr_aic_twoway$null.deviance),
  Null_DF=c(bank_lr$df.null,bank_lr_aic$df.null,bank_lr_bic$df.null,bank_lr_plotobs$df.null,bank_lr_aic$df.null,bank_lr_aic_twoway$df.null),
  AIC=c(AIC(bank_lr),AIC(bank_lr_aic),AIC(bank_lr_bic),AIC(bank_lr_plotobs),AIC(bank_lr_aic),AIC(bank_lr_aic_twoway)),
  MisclassificationRate=c(mcRate_bank_lr,mcRate_bank_lr_aic,mcRate_bank_lr_bic,mcRate_bank_lr_plotobs,mcRate_bank_lr_aic_twoway,mcRate_bank_lr_aic_transform),
  row.names = c("Full Model","Model Selected with AIC","Model Selected with BIC","Model built based on boxplot observation","Model with an interaction","Model with transformation")
)

knitr::kable(result,format="markdown")

```

<span style="color:Blue">As you see, Residual deviance goes down with transformation but AIC goes up. With interaction we dont see much difference. So at this point we can consider model selected with AIC model as preferred model. </span>



<span style="color:Blue">
** With Saying that, whether customer will do the term deposit or not does not depends on Age, Education and Marital status but more on below parameters.**
</span>

<span style="color:Blue">
1) Customer's credit history  - default
2) How frequent and when the customer was contacted by marketing team? - month, days of week, duration, pdays, previous
3) output of the last campain - poutcome
4) Market indicators - emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m

Let's run this model on test data to identify misclassification rate, false positives and negatives on Test data.

```{r}
predict_bank_lr_aic = ifelse(predict(bank_lr_aic, test, type = "response")> 0.5,1,0)
misClassificationRate = mean(predict_bank_lr_aic!=test$y)
misClassificationRate

make_conf_mat = function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

confusion_matrix = table(predicted = predict_bank_lr_aic, actual = test$y)
sensitivity = confusion_matrix[2,2]/confusion_matrix[,2]
specificity = confusion_matrix[1,1]/confusion_matrix[,1]

confusion_matrix
sensitivity
specificity

```

<span style="color:Blue"> With cutoff of 0.5 probability , we see `r confusion_matrix[2,1]` false positives and `r confusion_matrix[1,2]` false positives among 16000 observations which is less than 10%. We can try to identify cut off where this number goes down. Let's draw ROC curve and identify AUC.</span>

```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}

test_pred_10 = get_logistic_pred(bank_lr_aic, data = test, res = "default", 
                                 pos = 1, neg = 0, cut = 0.1)
test_pred_50 = get_logistic_pred(bank_lr_aic, data = test, res = "default", 
                                 pos = 1, neg = 0, cut = 0.5)
test_pred_90 = get_logistic_pred(bank_lr_aic, data = test, res = "default", 
                                 pos = 1, neg = 0, cut = 0.9)

test_tab_10 = table(predicted = test_pred_10, actual = test$y)
test_tab_50 = table(predicted = test_pred_50, actual = test$y)
test_tab_90 = table(predicted = test_pred_90, actual = test$y)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = "1")
test_con_mat_50 = confusionMatrix(test_tab_50, positive = "1")
test_con_mat_90 = confusionMatrix(test_tab_90, positive = "1")

metrics = rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])

)

rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
metrics
```

```{r}


test_prob = predict(bank_lr_aic, newdata = test, type = "response")
test_roc = roc(test$y ~ test_prob, plot = TRUE, print.auc = TRUE)

```
<span style="color:blue"> Getting higher AUC as 0.934 so that indicates higher specificity and sensitivity. That means selected model is a good model</span>


####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
####  Plot ROC curve (TPR vs FPR), AIC plot, RMSE, adjr2
####  Pick best using AIC,BIC, RSS, RMSE, and anova Test
####  Report Accuracy

### Model 2: Random Forest
####  model using all predictors
```{r }
model_rf<-randomForest(y ~ .  , data=train, mtry=3)
```

```{r }
model_rf
```

```{r }
varImpPlot(model_rf)
```

####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
```{r }
pred_train<-predict(model_rf,train)
confusionMatrix(train$y,pred_train)

pred_test1<-predict(model_rf,test)
confusionMatrix(test$y,pred_test1)

```

####  Plot ROC curve (TPR vs FPR)
```{r }
par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")


```

####  Tuning Random Forest Model
```{r }
oob = trainControl(method = "oob")
cv_5 = trainControl(method = "cv", number = 5)

rf_mtry = ceiling(sqrt(21))
rf_grid =  expand.grid(mtry = 1:rf_mtry)
set.seed(1)
bank_rf_tune = train(y ~ . , data = train,
                     method = "rf",
                     trControl = oob,
                     verbose = FALSE,
                     tuneGrid = rf_grid)
bank_rf_tune
```

####  Report Accuracy

```{r }

model_rf_best<-randomForest(y ~ .  , data=train, mtry=bank_rf_tune$bestTune$mtry[[1]])

```

```{r }
pred_train<-predict(model_rf_best,train)
pred_test1<-predict(model_rf_best,test)
train_cm = confusionMatrix(train$y,pred_train)
test1_cm = confusionMatrix(test$y,pred_test1)

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


### Model 3: ADAPTIVE BOOSTING

- AdaBoost is an ensemble learning method (also known as “meta-learning”) which was initially created to increase the efficiency of binary classifiers. AdaBoost uses an iterative approach to learn from the mistakes of weak classifiers, and turn them into strong ones. We will be using the library `ada` for the same. This model have built in feature selection.


- Build the model with default parameter values

```{r message=FALSE, warning=FALSE}
bank_model_ada = ada(y ~ ., data=train,loss='exponential', type='discrete', iter = 50)
```

```{r }
bank_model_ada$confusion
(bank_model_ada$confusion[2,2]/sum(bank_model_ada$confusion[,2]))
```

- Below plot shows training error vs iteration where training error reduces and stabilizes as iteration grows
```{r }
plot(bank_model_ada)
```

- Plot of variables ordered by the variable importance measure 

```{r }
varplot(bank_model_ada)
```

- In the above training error plot the error rate reduced at minimal ~25 and then slightly increased. So lets try with lesser iterations and measure the metrics

```{r ,message=FALSE, warning=FALSE}
bank_model_ada25 = ada(y ~ ., data=train,loss='exponential', type='discrete', iter = 25)
```

```{r }
bank_model_ada25$confusion
(bank_model_ada25$confusion[2,2]/sum(bank_model_ada25$confusion[,2]))
```


```{r }
bank_model_ada100 = ada(y ~ ., data=train,loss='exponential', type='discrete', iter = 100)
```

```{r }
bank_model_ada100$confusion
(bank_model_ada100$confusion[2,2]/sum(bank_model_ada100$confusion[,2]))
```

```{r }
plot(bank_model_ada100)
```

- Since our dataset has class imbalance problem where less positive cases and lot of negative cases. Lets use resampled training data using `SMOTE` and `ROSE` function for training.

```{r message=FALSE, warning=FALSE}
model_ada_smote = ada(y ~ ., data=smote_train,loss='exponential', type='discrete',iter=100)

model_ada_smote$confusion
(model_ada_smote$confusion[2,2]/sum(model_ada_smote$confusion[,2]))

model_ada_rose = ada(y ~ ., data=rose_train,loss='exponential', type='discrete',iter=100)

model_ada_rose$confusion
(model_ada_rose$confusion[2,2]/sum(model_ada_rose$confusion[,2]))

```

- We see that our accuracy increased after our using resampled data using smote and rose function.

- Plot TPR VS FPR for Training and Test Dataset

```{r }
pred_train = predict(bank_model_ada100, train)
train_cm = confusionMatrix(train$y,as.factor(pred_train))
pred_test = predict(bank_model_ada100, test)
test1_cm=confusionMatrix(test$y,as.factor(pred_test))

par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

```


- Metrics for Training VS Testing Dataset

```{r }
auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Plot TPR vs FPR (SMOTED TRAIN vs TEST)

```{r }
pred_train = predict(model_ada_smote, smote_train)
train_cm = confusionMatrix(smote_train$y,as.factor(pred_train))
pred_test1 = predict(model_ada_smote, test)
test1_cm=confusionMatrix(test$y,as.factor(pred_test1))

par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),smote_train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)
```

- Metrics for SMOTED TRAIN vs TEST

```{r }
auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

- Plot TPR vs FPR (ROSE TRAIN vs TEST)

```{r }
pred_train = predict(model_ada_rose, rose_train)
train_cm = confusionMatrix(rose_train$y,as.factor(pred_train))
pred_test1 = predict(model_ada_rose, test)
test1_cm=confusionMatrix(test$y,as.factor(pred_test1))

par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),rose_train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

```

- Metrics for ROSE Train vs Test

```{r }
auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


## Results

### Compare Models (Test and Train Accuracy)


## Discussion

- Do we accept False Positive or False Negative in the context of the problem statement?

    + False Positive, means the client do NOT SUBSCRIBED to term deposit, but the model thinks he/she did. 
  
**Type I error is more harmful because we might lose the customer and loss revenue by incorrectly considering the client was already subscribed.**

    + False Negative, means the client SUBSCRIBED to term deposit, but the model thinks otherwise.
    
**Type II error is less harmful because we didn't lose the customer and no loss in revenue by incorrectly considering the client was not subscribed. We may be contacting the customer for the same campaign but it won't cause to lose a customer **


- Flexibility vs Interpretability - what do we choose?

    + Smaller model(Logistic Regression) are easily interpretable with decent accuracy in prediction.
    
    + Complex, flexible model (Random Forest, Adaptive Boosting) are less interpretable but with high accuracy.
    
**In this study, we must go with complex and flexible model eventhough we lose interpretability in the expense for higher prediction since it involves revenue to the bank.**


- Speed - **We need to choose the model which performs better and run faster. Boosting models run in parallel compared to Logistic and Random Forest.**

- With the above traits for model evaluation and selection techniques, we choose "Adaptive Boosting" from our study. 

## Conclusion

- In this study, we have applied regression and advanced machine learning techniques on bank marketing data and explored the distribution of data, data curation and modeling techniques. The below are few key highlights.

- Multiple Classification and Regression Algorithms have been evaluated.
- Garbage in - Garbage out so we analyzed the data and curated data to further analysis.
- Oversampling/undersampling has been implemented for class imbalanced data.
- Cross Validation was used for parameter selection with logistic regression.
- Random Forest used with default and best split and tree depth hyper parameters tuned to produce better result.
- Adaptive Boosting is the best model for balanced sensitivities and interpretation of feature importance.
- Our study shows that simple and advanced machine learning techniques can add value to further a marketing campaign.
 


## Appendix

### Team members

- Bhusan Bathani (bbath2)
- Mathew Leung (wmleung2)
- Vijayakumar Sitha Mohan (VS24)

### Source Repository

- [Our Data Analysis Project Source](https://github.com/smvijaykumar/stat420-group-project)


### UCI machine Learning Library:

- [Bank Marketing Dataset Source](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing)

- [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014
