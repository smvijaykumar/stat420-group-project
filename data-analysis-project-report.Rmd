---
title: 'Bank Marketing Data Analysis - Project Report'
author: "STAT 420, Summer 2020, Bhushan Bhathani, Matthew Leung, Vijayakumar Sitha Mohan,"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
#install.packages("e1071")
library(knitr)      # web widget
library(tidyverse)  # data manipulation
library(data.table) # fast file reading
library(caret)      # rocr analysis
library(ROCR)       # rocr analysis
library(kableExtra) # nice table html formating 
library(gridExtra)  # arranging ggplot in grid
library(rpart)      # decision tree
library(rpart.plot) # decision tree plotting
library(caTools)    # split
library(lmtest)
library(randomForest)
library(ada)
library(boot)
library(e1071)      # caret depends on this
library(ada)
library(ROSE)
library(DMwR)
library(latticeExtra)
library(pROC)
```

## Introduction

Our team selected a dataset of direct banking marketing of a Portuguese bank. The goal is to predict if the client subscribe to the bank term deposit. The dataset has client personal attributes, marketing campaign attributes, social and economic context attributes. The response variable (y) is binomial with value either 1 (subscribe) or 0 (not subscribe).

Our team is interested in various classification technique performance. We selected this dataset becasue it is a clean dataset for classification. It has been top ten most popular from the data repository that we download from. We can focus on analysing classification techniques rather then cleaning data. The URL to the dataset is http://archive.ics.uci.edu/ml/datasets/Bank+Marketing.

We pick the following three classification techniques for this project:
- Logistic Regression (It is covered in this course STAT420)
- Random Forest (Very popular technique in Ensemble learning with a number of weak classifier based on decision tree)
- Adaptive Boosting (More sofisticated technique in Ensemble learning with a number of weak classifier based on decision tree)


## Methods

```{r echo=FALSE}
bank_df1 = read.csv("data/bank-additional.csv")
```

### Understanding Data

- The Bank-Marketing dataset is downloaded from UCI Machine Learning Repository and the same is available at http://archive.ics.uci.edu/ml/datasets/Bank+Marketing.
- There were 4 datasets in it from which bank-additional-full.csv is used that has all examples (41188) and
20 inputs ordered by date (from May 2008 to November 2010). There are 20 input variables and 1 output
variable (desired target). 
- The dataset has  customer data, socio-economic data, telemarketing data and some other data. Some attributes are numerical, and some are categorical. 

**Attribute Information:**

```{r echo=FALSE, message=FALSE, warning=FALSE}
columns = read.csv("data/col_names.csv")
kable(columns) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Output variable (desired target) is **Term Deposit** which is categorical binary variable.

#### Data Validation

**Checking for Missing value**
```{r}
any(is.na(bank_df1))
sum(is.na(bank_df1))
```

#### Data Curation  (cleaning , missing data, duplicate)

**Remove Duplicates**

```{r}
bank_df = bank_df1 %>% distinct
```

**Recode categorical attributes as factor variables**
```{r}
bank_df$y = ifelse(bank_df$y=='yes',1,0)
bank_df$y = as.factor(bank_df$y)
bank_df$job = as.factor(bank_df$job)
bank_df$education = as.factor(bank_df$education)
bank_df$marital = as.factor(bank_df$marital)
bank_df$default = as.factor(bank_df$default)
bank_df$housing = as.factor(bank_df$housing)
bank_df$loan = as.factor(bank_df$loan)
bank_df$contact = as.factor(bank_df$contact)
bank_df$month = as.factor(bank_df$month)
bank_df$poutcome = as.factor(bank_df$poutcome)
bank_df$day_of_week = as.factor(bank_df$day_of_week)
```

**Verify Levels of Categorical attributes**

```{r}
levels(bank_df$job)
levels(bank_df$marital)
levels(bank_df$education)
levels(bank_df$default)
levels(bank_df$housing)
levels(bank_df$loan)
levels(bank_df$contact)
levels(bank_df$month)
levels(bank_df$poutcome)
```

#### Data Analysis


```{r}
pairs_df = subset(bank_df, select = c(1, 10, 11, 13:17))
pairs(pairs_df, pch = 19, gap = .25, xaxt = "n", yaxt = "n"
      , col = c("#0080FF", "#F3953E")[bank_df$y]
      , label.pos = .5, oma = c(1, 1, 1, 1))
```


**Attribute wise Frequency Distribution**

```{r}
par(mfrow=c(2,2))
for(i in 1:21){
  barplot(prop.table(table(bank_df[,i])) , 
           xlab=names(bank_df[i]), ylab= "Frequency (%)" , col = rainbow(3), horiz = FALSE)
  }
```


#### Class Imbalance Problem analysis

- We say that the dataset have the class imbalance problem, where the main class of interest
is rare. That is, the data set distribution reflects a significant majority of the
negative class and a minority positive class. Below table investigation shows that. we have `88.8%` negative classes and `11.2%` positive classes.

```{r}
prop.table(table(bank_df$y))
```

- we need to address this problem otherwise our model won't able to predict correctly. Most of the traditional classifiers assume dataset have balanced class distribution.

- There are techniques available to overcome this class imbalance problem. Lets consider few of those techniques are:
    + Oversampling: Re-sampling of data from positive class
    
    + Under-sampling: Randomly eliminate tuples from negative class
    
    + Threshold-moving: Move the decision threshold, so that the rare class tuples are easier to classify, and hence, less chance of costly false negative errors.
    
- We have used two library packages `DMwR` and `ROSE` which has `SMOTE` and `ROSE` functions

    + SMOTE: The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset.
    
    + ROSE: ROSE (Random Over-Sampling Examples) aids the task of binary classification in the presence of rare classes. It produces a synthetic, possibly balanced, sample of data simulated according to a smoothed-bootstrap approach.
    
#### Train/Test data split

- First split the dataset into train and test split using `createDataPartition` from `caret` package. The below r code splits the dataset with 70:30 ratio.

```{r}
split<-createDataPartition(bank_df$y,p=0.7,list = FALSE)
train<-bank_df[split,]
test<-bank_df[-split,]
```

- Lets verify the class distribution in Training and Testing dataset. We see that our training and test dataset have same distribution of negative and positive classes.

```{r}
prop.table(table(train$y))
prop.table(table(test$y))
```

- Lets apply the class imbalances improvement techniques which we discussed above and verify the results.

```{r}
smote_train <- SMOTE(y ~ ., data = train)                         
rose_train <- ROSE(y ~ ., data = train)$data                   
```


```{r}
prop.table(table(smote_train$y))
prop.table(table(rose_train$y))
```


## Model Builing
### Model 1: Logistic Regression
####  model using all predictors

<span style="color:Blue"> There are 3 kinds of data in the dataset.
1)Customer's data and  - age,job,marital,education,default,housing,loan
2)Their financial data - default, hosing loan, personal loan
3)Campaign and Marketing related data - contact,month,day_of_week,duration,campaign,pdays,previous,poutcome
4) Overall market data which indicates how well the economy is - emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed

Let's analyze each dataset and see if we see any linearity between predictors and y using boxplot and pairs.

</span>


```{r}
pairs(train[c(1:4,21)])
par(mfrow=c(2,2))
bank_df$jobclass = as.numeric(factor(bank_df$job,levels=c("unknown","unemployed","housemaid", "student","retired","technician","blue-collar","admin.","services","self-employed","management","entrepreneur"),ordered=TRUE))

bank_df$educationNum = as.numeric(factor(bank_df$education,levels=c("unknown","illiterate","basic.4y","basic.6y","basic.9y","high.school","professional.course", "university.degree"),ordered=TRUE))

boxplot(age~y,data=bank_df,main="age vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(jobclass~y,data=bank_df,main="Job vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(marital)~y,data=bank_df,main="Marital status vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(educationNum~y,data=bank_df,main="Education vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```
<span style="color:Blue"> Based on a bar plot, we can see most of the people are between an age of 30 and 50 years who are a part of dataset and more number of people in that age group  subscribes for term deposit than not.Same amount of People with an education higher than high school takes the loan and subscribes for term deposit vs not.Most of the people are married and single who are in dataset and seems same amount of people who subscribes for term deposit vs not. Most of the people with jobclass as technician, blue-color, admin and services sector is in dataset and same amount fo people subscribes for term deposit vs not. So over all, seems age has some impact on term deposit but need to see which exact range between 30-50 in detail. Rest parameters does not have any significant effect based on plot.</span>

```{r}
levels(bank_df$education)
pairs(train[c(5:7,21)])
par(mfrow=c(1,3))
boxplot(as.numeric(default)~y,data=bank_df,main="Default vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(housing)~y,data=bank_df,main="Housing Loan vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(loan)~y,data=bank_df,main="Personal loan vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```
<span style="color:Blue">People with the housing loan and personal loan has no impact on term deposit subscription. But whoever is default is certainly not subscribed for term deposit. So default can be a very good criteria.</span>
```{r}
pairs(train[c(8:15,21)])
par(mfrow=c(2,2))

levels(bank_df$contact)
boxplot(as.numeric(contact)~y,data=bank_df,main="Contact Type vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(month)~y,data=bank_df,main="last contact month of year vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(day_of_week)~y,data=bank_df,main=" last contact day of the week vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(duration~y,data=bank_df,main="last contact duration, in seconds vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```
<span style="color:Blue">Based on plots, Customer with contact type cellular has more probability of subscribing for term deposit than telephone. People with recent contact made has more chance of subscribing for term deposit. Last contact day of the week does not have much ipact.Contact duraction has some impact on the term deposit subscription as well.</span>

```{r}
levels(bank_df$poutcome)
par(mfrow=c(2,2))
boxplot(campaign~y,data=bank_df,main="# of contacts in this campaign vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(pdays~y,data=bank_df,main="number of days that passed by after the client was last contacted vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(previous~y,data=bank_df,main="number of contacts performed before this campaign vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(as.numeric(poutcome)~y,data=bank_df,main=" Previous Campain outcome vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```
<span style="color:Blue">Based on above plot, # of contact performed has some impact on Term deposit output else no other factor has significant impact.surprisingly # of contacts in this campaign decreases the probability to subscribe for term deposit.</span>
```{r}
pairs(train[c(16:20,21)])
par(mfrow=c(3,2))
boxplot(emp.var.rate~y,data=bank_df,main="employment variation rate  vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(cons.price.idx~y,data=bank_df,main="consumer price index vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(cons.conf.idx~y,data=bank_df,main="consumer confidence index  vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(euribor3m~y,data=bank_df,main="euribor 3 month rate  vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
boxplot(nr.employed~y,data=bank_df,main="number of employees vs Term Deposit",pch=20,cex=2,col="darkorange",border = "dodgerblue")
```

<span style="color:Blue"> Based on plots all the market indicators has some impact on term deposit decision.</span>

<span style="color:Blue"> So overall seems age, default, contact type, last contact time, duration, # of contacts and all market indicators has some impact on terms deposit. Let's create a additive model and run AIC and BIC search to identify the model it comes up with.</span>



```{r}
bank_lr_plotobs = glm(y~age+default+campaign+previous+emp.var.rate+cons.price.idx+cons.conf.idx+euribor3m+nr.employed, data = train, family = "binomial")
bank_lr = glm(y ~ ., data = train, family = "binomial")
summary(bank_lr)
```

<span style="color:Blue"> Going through AIC and BIC backward search to find out the best model.</span>

```{r warning=FALSE}
n=length(resid(bank_lr))
bank_lr_aic = step(bank_lr, direction = "backward", trace = 0)
bank_lr_bic = step(bank_lr, direction = "backward", trace = 0,k=log(n))
bank_lr_aic
bank_lr_bic
```
<span style="color:Blue"> BIC comes with least number of parameters. Let's do the anova LRT test to find the preferred model. </span>

```{r warning=FALSE}
result = data.frame(
        pvalue = c((anova(bank_lr_bic,bank_lr,test="LRT")[2,5]<0.05),
           (anova(bank_lr_aic,bank_lr,test="LRT")[2,5]<0.05),
           (anova(bank_lr_bic,bank_lr_aic,test="LRT")[2,5]<0.05),
           (anova(bank_lr_plotobs,bank_lr,test="LRT")[2,5]<0.05),
           (anova(bank_lr_plotobs,bank_lr_bic,test="LRT")[2,5]<0.05),
           (anova(bank_lr_plotobs,bank_lr_aic,test="LRT")[2,5]<0.05)),
row.names = c("BIC vs Additive","AIC vs Additive ","BIC vs AIC","PlotObs vs Full Additive","PlotObs vs BIC", "PlotObs vs AIC")
)



knitr::kable(result,format="markdown")

```

<span style="color:Blue">Based on Anova test above, only AIC model is the preferable model among all model.</span>

<span style="color:Blue">Let's check $R^2$ ,RSS and 5 fold misclassification rate to see which is the better model. </span>


```{r warning=FALSE}


r2_bank_lr = with(summary(bank_lr), 1 - (deviance/null.deviance))
r2_bank_lr_aic = with(summary(bank_lr_aic), 1 - (deviance/null.deviance))
r2_bank_lr_bic = with(summary(bank_lr_bic), 1 - (deviance/null.deviance))
r2_bank_lr_plotobs = with(summary(bank_lr_plotobs), 1 - (deviance/null.deviance))

rss_bank_lr=sum(resid(bank_lr)^2)
rss_bank_lr_aic=sum(resid(bank_lr_aic)^2)
rss_bank_lr_bic=sum(resid(bank_lr_bic)^2)
rss_bank_lr_plotobs = sum(resid(bank_lr_plotobs)^2)

mcRate_bank_lr = cv.glm(train,bank_lr,K=5)$delta[1]
mcRate_bank_lr_aic = cv.glm(train,bank_lr_aic,K=5)$delta[1]
mcRate_bank_lr_bic = cv.glm(train,bank_lr_aic,K=5)$delta[1]
mcRate_bank_lr_plotobs = cv.glm(train,bank_lr_plotobs,K=5)$delta[1]



result = data.frame(
  R2=c(r2_bank_lr,r2_bank_lr_aic,r2_bank_lr_bic,r2_bank_lr_plotobs),
  RSS=c(rss_bank_lr,rss_bank_lr_aic,rss_bank_lr_bic,rss_bank_lr_plotobs),
  MisclassificationRate=c(mcRate_bank_lr,mcRate_bank_lr_aic,rss_bank_lr_bic,mcRate_bank_lr_plotobs),
  row.names = c("Full Model","Model Selected with AIC","Model Selected with BIC","Model built based on boxplot observation")
)


knitr::kable(result,format="markdown")

```
<span style="color:Blue">  Based on the table above, AIC model's R^2 is almost highest after Full model. RSS of AIC model is also least after full model with very less difference. Misclassification rate is also small. But still it is not as good as full model. Can we improve it further by applying the transformation or interaction on AIC model? Let's check.</span>

```{r}

#bank_lr_aic_twoway = glm(y ~ (job + education + default + contact + month + 
 #   day_of_week + duration + campaign + pdays + poutcome + emp.var.rate + 
 #   cons.price.idx + cons.conf.idx + euribor3m + nr.employed)^2, 
 #   family = binomial(link="logit"), data = bank_df)

#anova(bank_lr_aic_twoway,bank_lr_aic,test="LRT")

qqnorm(resid(bank_lr_aic))
qqline(resid(bank_lr_aic), col = "dodgerblue", lwd = 2)

plot(fitted(bank_lr_aic), resid(bank_lr_aic), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "AIC Model Fitted vs Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
```





<span style="color:Blue">
** With Saying that, whether customer will do the term deposit or not does not depends on Age, Education and Marital status but more on below parameters.**
</span>

<span style="color:Blue">
1) Customer's credit history  - default
2) How frequent and when the customer was contacted by marketing team? - month, days of week, duration, pdays, previous
3) output of the last campain - poutcome
4) Market indicators - emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m

Let's run this model on test data to identify misclassification rate, false positives and negatives on Test data.

```{r}
predict_bank_lr_aic = ifelse(predict(bank_lr_aic, test, type = "response")> 0.5,1,0)
misClassificationRate = mean(predict_bank_lr_aic!=test$y)
misClassificationRate

make_conf_mat = function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

confusion_matrix = table(predicted = predict_bank_lr_aic, actual = test$y)
sensitivity = confusion_matrix[2,2]/confusion_matrix[,2]
specificity = confusion_matrix[1,1]/confusion_matrix[,1]

confusion_matrix
sensitivity
specificity

```

<span style="color:Blue"> With cutoff of 0.5 probability , we see `r confusion_matrix[2,1]` false positives and `r confusion_matrix[1,2]` false positives among 16000 observations which is less than 10%. We can try to identify cut off where this number goes down. Let's draw ROC curve and identify AUC.</span>

```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}

test_pred_10 = get_logistic_pred(bank_lr_aic, data = test, res = "default", 
                                 pos = 1, neg = 0, cut = 0.1)
test_pred_50 = get_logistic_pred(bank_lr_aic, data = test, res = "default", 
                                 pos = 1, neg = 0, cut = 0.5)
test_pred_90 = get_logistic_pred(bank_lr_aic, data = test, res = "default", 
                                 pos = 1, neg = 0, cut = 0.9)

test_tab_10 = table(predicted = test_pred_10, actual = test$y)
test_tab_50 = table(predicted = test_pred_50, actual = test$y)
test_tab_90 = table(predicted = test_pred_90, actual = test$y)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = "1")
test_con_mat_50 = confusionMatrix(test_tab_50, positive = "1")
test_con_mat_90 = confusionMatrix(test_tab_90, positive = "1")

metrics = rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])

)

rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
metrics
```

```{r}


test_prob = predict(bank_lr_aic, newdata = test, type = "response")
test_roc = roc(test$y ~ test_prob, plot = TRUE, print.auc = TRUE)

```
<span style="color:blue"> Getting higher AUC as 0.934 so that indicates higher specificity and sensitivity. That means selected model is a good model</span>


####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
####  Plot ROC curve (TPR vs FPR), AIC plot, RMSE, adjr2
####  Pick best using AIC,BIC, RSS, RMSE, and anova Test
####  Report Accuracy

### Model 2: Random Forest
####  model using all predictors
```{r }
model_rf<-randomForest(y ~ .  , data=train, mtry=3)
```

```{r }
model_rf
```

```{r }
varImpPlot(model_rf)
```

####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
```{r }
pred_train<-predict(model_rf,train)
confusionMatrix(train$y,pred_train)

pred_test1<-predict(model_rf,test)
confusionMatrix(test$y,pred_test1)

```

####  Plot ROC curve (TPR vs FPR)
```{r }
par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")


```

####  Tuning Random Forest Model
```{r }
oob = trainControl(method = "oob")
cv_5 = trainControl(method = "cv", number = 5)

rf_mtry = ceiling(sqrt(21))
rf_grid =  expand.grid(mtry = 1:rf_mtry)
set.seed(1)
bank_rf_tune = train(y ~ . , data = train,
                     method = "rf",
                     trControl = oob,
                     verbose = FALSE,
                     tuneGrid = rf_grid)
bank_rf_tune
```

####  Report Accuracy

```{r }

model_rf_best<-randomForest(y ~ .  , data=train, mtry=bank_rf_tune$bestTune$mtry[[1]])

```

```{r }
pred_train<-predict(model_rf_best,train)
pred_test1<-predict(model_rf_best,test)
train_cm = confusionMatrix(train$y,pred_train)
test1_cm = confusionMatrix(test$y,pred_test1)

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


### Model 3: ADAPTIVE BOOSTING

- AdaBoost is an ensemble learning method (also known as “meta-learning”) which was initially created to increase the efficiency of binary classifiers. AdaBoost uses an iterative approach to learn from the mistakes of weak classifiers, and turn them into strong ones. We will be using the library `ada` for the same. This model have built in feature selection.


- Build the model with default parameter values

```{r message=FALSE, warning=FALSE}
bank_model_ada = ada(y ~ ., data=train,loss='exponential', type='discrete', iter = 50)
```

```{r }
bank_model_ada$confusion
(bank_model_ada$confusion[2,2]/sum(bank_model_ada$confusion[,2]))
```

- Below plot shows training error vs iteration where training error reduces and stabilizes as iteration grows
```{r }
plot(bank_model_ada)
```

- Plot of variables ordered by the variable importance measure 

```{r }
varplot(bank_model_ada)
```

- In the above training error plot the error rate reduced at minimal ~25 and then slightly increased. So lets try with lesser iterations and measure the metrics

```{r ,message=FALSE, warning=FALSE}
bank_model_ada25 = ada(y ~ ., data=train,loss='exponential', type='discrete', iter = 25)
```

```{r }
bank_model_ada25$confusion
(bank_model_ada25$confusion[2,2]/sum(bank_model_ada25$confusion[,2]))
```


```{r }
bank_model_ada100 = ada(y ~ ., data=train,loss='exponential', type='discrete', iter = 100)
```

```{r }
bank_model_ada100$confusion
(bank_model_ada100$confusion[2,2]/sum(bank_model_ada100$confusion[,2]))
```

```{r }
plot(bank_model_ada100)
```

- Since our dataset has class imbalance problem where less positive cases and lot of negative cases. Lets use resampled training data using `SMOTE` and `ROSE` function for training.

```{r message=FALSE, warning=FALSE}
model_ada_smote = ada(y ~ ., data=smote_train,loss='exponential', type='discrete',iter=100)

model_ada_smote$confusion
(model_ada_smote$confusion[2,2]/sum(model_ada_smote$confusion[,2]))

model_ada_rose = ada(y ~ ., data=rose_train,loss='exponential', type='discrete',iter=100)

model_ada_rose$confusion
(model_ada_rose$confusion[2,2]/sum(model_ada_rose$confusion[,2]))

```

- We see that our accuracy increased after our using resampled data using smote and rose function.

- Plot TPR VS FPR for Training and Test Dataset

```{r }
pred_train = predict(bank_model_ada100, train)
train_cm = confusionMatrix(train$y,as.factor(pred_train))
pred_test = predict(bank_model_ada100, test)
test1_cm=confusionMatrix(test$y,as.factor(pred_test))

par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

```


- Metrics for Training VS Testing Dataset

```{r }
auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

- Plot TPR vs FPR (SMOTED TRAIN vs TEST)

```{r }
pred_train = predict(model_ada_smote, smote_train)
train_cm = confusionMatrix(smote_train$y,as.factor(pred_train))
pred_test1 = predict(model_ada_smote, test)
test1_cm=confusionMatrix(test$y,as.factor(pred_test1))

par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),smote_train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)
```

- Metrics for SMOTED TRAIN vs TEST

```{r }
auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

- Plot TPR vs FPR (ROSE TRAIN vs TEST)

```{r }
pred_train = predict(model_ada_rose, rose_train)
train_cm = confusionMatrix(rose_train$y,as.factor(pred_train))
pred_test1 = predict(model_ada_rose, test)
test1_cm=confusionMatrix(test$y,as.factor(pred_test1))

par(mfrow = c(1,2))
pr_train = prediction(as.numeric(pred_train),rose_train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

```

- Metrics for ROSE Train vs Test

```{r }
auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")

metrics = cbind(train_cm$byClass, test1_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values)
rownames(auc) = c("Area Under Curve")
result = data.frame(rbind(acc,auc,metrics))

colnames(result) = c("Train","Test")
kable(result) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


## Results

### Compare Models (Test and Train Accuracy)


## Discussion

- Do we accept False Positive or False Negative in the context of the problem statement?

    + False Positive, means the client do NOT SUBSCRIBED to term deposit, but the model thinks he/she did. 
  
**Type I error is more harmful because we might lose the customer and loss revenue by incorrectly considering the client was already subscribed.**

    + False Negative, means the client SUBSCRIBED to term deposit, but the model thinks otherwise.
    
**Type II error is less harmful because we didn't lose the customer and no loss in revenue by incorrectly considering the client was not subscribed. We may be contacting the customer for the same campaign but it won't cause to lose a customer **


- Flexibility vs Interpretability - what do we choose?

    + Smaller model(Logistic Regression) are easily interpretable with decent accuracy in prediction.
    
    + Complex, flexible model (Random Forest, Adaptive Boosting) are less interpretable but with high accuracy.
    
**In this study, we must go with complex and flexible model eventhough we lose interpretability in the expense for higher prediction since it involves revenue to the bank.**


- Speed - **We need to choose the model which performs better and run faster. Boosting models run in parallel compared to Logistic and Random Forest.**

- With the above traits for model evaluation and selection techniques, we choose "Adaptive Boosting" from our study. 

## Conclusion

- In this study, we have applied regression and advanced machine learning techniques on bank marketing data and explored the distribution of data, data curation and modeling techniques. The below are few key highlights.

- Multiple Classification and Regression Algorithms have been evaluated.
- Garbage in - Garbage out so we analyzed the data and curated data to further analysis.
- Oversampling/undersampling has been implemented for class imbalanced data.
- Cross Validation was used for parameter selection with logistic regression.
- Random Forest used with default and best split and tree depth hyper parameters tuned to produce better result.
- Adaptive Boosting is the best model for balanced sensitivities and interpretation of feature importance.
- Our study shows that simple and advanced machine learning techniques can add value to further a marketing campaign.
 


## Appendix

### Team members

- Bhusan Bathani (bbath2)
- Mathew Leung (wmleung2)
- Vijayakumar Sitha Mohan (VS24)

### Source Repository

- [Our Data Analysis Project Source](https://github.com/smvijaykumar/stat420-group-project)


### UCI machine Learning Library:

- [Bank Marketing Dataset Source](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing)

- [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014

