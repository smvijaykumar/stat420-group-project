---
title: 'Bank Marketing Data Analysis - Project Report'
author: "STAT 420, Summer 2020, Bhushan Bhathani, Matthew Leung, Vijayakumar Sitha Mohan,"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE}
#install.packages("e1071")
library(knitr)      # web widget
library(tidyverse)  # data manipulation
library(data.table) # fast file reading
library(caret)      # rocr analysis
library(ROCR)       # rocr analysis
library(kableExtra) # nice table html formating 
library(gridExtra)  # arranging ggplot in grid
library(rpart)      # decision tree
library(rpart.plot) # decision tree plotting
library(caTools)    # split
library(lmtest)
library(randomForest)
library(ada)
library(boot)
library(e1071)      # caret depends on this
```


## Introduction

Our team selected a dataset of direct banking marketing of a Portuguese bank. The goal is to predict if the client subscribe to the bank term deposit. The dataset has client personal attributes, marketing campaign attributes, social and economic context attributes. The response variable (y) is binomial with value either 1 (subscribe) or 0 (not subscribe).

Our team is interested in various classification technique performance. We selected this dataset becasue it is a clean dataset for classification. It has been top ten most popular from the data repository that we download from. We can focus on analysing classification techniques rather then cleaning data. The URL to the dataset is http://archive.ics.uci.edu/ml/datasets/Bank+Marketing.

We pick the following three classification techniques for this project:
- Logistic Regression (It is covered in this course STAT420)
- Random Forest (Very popular technique in Ensemble learning with a number of weak classifier based on decision tree)
- Adaptive Boosting (More sofisticated technique in Ensemble learning with a number of weak classifier based on decision tree)


## Methods

```{r echo=FALSE}
bank_df1 = read.csv("data/bank-additional-full.csv")
```

### Understanding Data
#### Data Validation
```{r}
any(is.na(bank_df1))
sum(is.na(bank_df1))
```

#### Data Curation  (cleaning , missing data, duplicate)
```{r}
bank_df = bank_df1 %>% distinct
#bank_df = bank_df %>% filter(housing == "yes" & housing == "no")

bank_df$y = ifelse(bank_df$y=='yes',1,0)
bank_df$y = as.factor(bank_df$y)
bank_df$job = as.factor(bank_df$job)
bank_df$education = as.factor(bank_df$education)
bank_df$marital = as.factor(bank_df$marital)
bank_df$default = as.factor(bank_df$default)
bank_df$housing = as.factor(bank_df$housing)
bank_df$loan = as.factor(bank_df$loan)
bank_df$contact = as.factor(bank_df$contact)
bank_df$month = as.factor(bank_df$month)
bank_df$poutcome = as.factor(bank_df$poutcome)
```

```{r}
levels(bank_df$job)
levels(bank_df$marital)
levels(bank_df$education)
levels(bank_df$default)
levels(bank_df$housing)
levels(bank_df$loan)
levels(bank_df$contact)
levels(bank_df$month)
levels(bank_df$poutcome)
View(bank_df)
```

#### Data Analysis  (attribute wise distribution, subscription plot analysis)
```{r}
par(mfrow=c(2,2))
for(i in 1:length(bank_df)){
  barplot(prop.table(table(bank_df[,i])) , 
           xlab=names(bank_df[i]), ylab= "Frequency (%)" , col = rainbow(3))
  }
```


#### Class Imbalance Problem analysis
```{r}
prop.table(table(bank_df$y))
```
#### Train/Test data split (stratified cv)

- Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the data such that in every fold, each class comprises of about half the instances.

```{r}
split<-createDataPartition(bank_df$y,p=0.6,list = FALSE)
train<-bank_df[split,]
test<-bank_df[-split,]
split<-createDataPartition(test$y,p=0.5,list=FALSE)
test1<-test[split,]
test2<-test[-split,]

```

```{r}
prop.table(table(train$y))
prop.table(table(test1$y))
prop.table(table(test2$y))
```


## Model Builing
### Model 1: Logistic Regression
####  model using all predictors
```{r}
bank_lr = glm(y ~ ., data = train, family = "binomial")
summary(bank_lr)
```

<span style="color:Blue"> Going through AIC and BIC forward search to find out the best model.</span>

```{r warning=FALSE}
bank_lr_aic = step(bank_lr, direction = "backward", trace = 0)
bank_lr_bic = step(bank_lr, direction = "backward", trace = 0)
bank_lr_aic
bank_lr_bic
```
<span style="color:Blue"> AIC and BIC both find s the same model. Let's do the anova test to compare it with full model. </span>

```{r warning=FALSE}
anova(bank_lr_aic,bank_lr,test="LRT")
```

<span style="color:Blue">Based on Anova test, null hypothesis is failed to reject so smaller aic model is the better model. Let's check $R^2$ ,RSS and 5 fold missclassification rate to see which is a better model. </span>


```{r warning=FALSE}


r2_bank_lr = with(summary(bank_lr), 1 - (deviance/null.deviance))
r2_bank_lr_aic = with(summary(bank_lr_aic), 1 - (deviance/null.deviance))
rss_bank_lr=sum(resid(bank_lr)^2)
rss_bank_lr_aic=sum(resid(bank_lr_aic)^2)
mcRate_bank_lr = cv.glm(train,bank_lr,K=5)$delta[1]
mcRate_bank_lr_aic = cv.glm(train,bank_lr_aic,K=5)$delta[1]



result = data.frame(
  R2=c(r2_bank_lr,r2_bank_lr_aic),
  RSS=c(rss_bank_lr,rss_bank_lr_aic),
  MisclassificationRate=c(mcRate_bank_lr,mcRate_bank_lr_aic),
  row.names = c("Full Model","Model Selected with AIC/BIC")
)


knitr::kable(result,format="markdown")

```
<span style="color:Blue"> There is no major difference between both model's $R^2$ , RSS and Misclassification Rate. So full model is overfittig and Model searched with AIC/BIC forward search fits well with the result.** With Saying that, whether customer will do the term deposit or not does not depends on Age, Education and Marital status but more on below parameters.**

1) Job type of the customer
2) Customer's credit history  - default
2) Any other debt/loan information - housing/personal loan.
3) How frequent and when the customer was contacted by marketing team? - month, days of week, duration, pdays, previous
4) output of the last campain - poutcome
5) Market indicators - emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m

Let's run this model on test data to identify misclassification rate, false positives and negatives on Test data.

</span>

```{r}
predict_bank_lr_aic = ifelse(predict(bank_lr_aic, test, type = "response")> 0.5,1,0)
misClassificationRate = mean(predict_bank_lr_aic!=test$y)
misClassificationRate

make_conf_mat = function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

confusion_matrix = table(predicted = predict_bank_lr_aic, actual = test$y)
sensitivity = confusion_matrix[2,2]/confusion_matrix[,2]
specificity = confusion_matrix[1,1]/confusion_matrix[,1]

confusion_matrix
sensitivity
specificity

```

<span style="color:Blue"> With cutoff of 0.5 probability , we see 365 false positives and 1069 false positives among 16000 observations which is less than 10%. We can try to identify cut off where this number goes down.</span>



####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
####  Plot ROC curve (TPR vs FPR), AIC plot, RMSE, adjr2
####  Pick best using AIC,BIC, RSS, RMSE, and anova Test
####  Report Accuracy

### Model 2: Random Forest
####  model using all predictors
```{r}
model_rf<-randomForest(y ~ .  , data=train, mtry=3)
```

```{r}
model_rf
```

```{r}
varImpPlot(model_rf)
```

####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
```{r}
pred_train<-predict(model_rf,train)
confusionMatrix(train$y,pred_train)

pred_test1<-predict(model_rf,test1)
confusionMatrix(test1$y,pred_test1)

pred_test2<-predict(model_rf,test2)
confusionMatrix(test2$y,pred_test2)
```

####  Plot ROC curve (TPR vs FPR)
```{r}
par(mfrow = c(1,3))
pr_train = prediction(as.numeric(pred_train),train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test1$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

pr_test2 = prediction(as.numeric(pred_test2),test2$y)
prf_test2 = performance(pr_test2, measure = "tpr",x.measure = "fpr")
plot(prf_test2)

auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")
auc_test2 = performance(pr_test2, measure = "auc")


```

####  Tuning Random Forest Model
```{r}
oob = trainControl(method = "oob")
cv_5 = trainControl(method = "cv", number = 5)

rf_mtry = ceiling(sqrt(21))
rf_grid =  expand.grid(mtry = 1:rf_mtry)
set.seed(1)
bank_rf_tune = train(y ~ . , data = train,
                     method = "rf",
                     trControl = oob,
                     verbose = FALSE,
                     tuneGrid = rf_grid)
bank_rf_tune
```

####  Report Accuracy

```{r}

model_rf_best<-randomForest(y ~ .  , data=train, mtry=bank_rf_tune$bestTune$mtry[[1]])

```

```{r}
pred_train<-predict(model_rf_best,train)
pred_test1<-predict(model_rf_best,test1)
pred_test2<-predict(model_rf_best,test2)
train_cm = confusionMatrix(train$y,pred_train)
test2_cm = confusionMatrix(test2$y,pred_test2)
test1_cm = confusionMatrix(test1$y,pred_test1)

metrics = cbind(train_cm$byClass, test1_cm$byClass,test2_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"], test2_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values,auc_test2@y.values)
rownames(auc) = c("Area Under Curve")
result = rbind(acc,auc,metrics)

colnames(result) = c("Train","Test1","Test2")
kable(result)
```


### Model 3: ADAPTIVE BOOSTING
```{r message=FALSE, warning=FALSE}
library(ada)
model_ada = ada(y ~ ., data=train,loss='exponential', type='discrete',iter=100)
model_ada

```

```{r}
plot(model_ada)
```


```{r}
pred_train = predict(model_ada, train)
train_cm = confusionMatrix(train$y,as.factor(pred_train))
pred_test1 = predict(model_ada, test1)
test1_cm=confusionMatrix(test1$y,as.factor(pred_test1))
pred_test2 = predict(model_ada, test2)
test2_cm=confusionMatrix(test2$y,as.factor(pred_test2))

par(mfrow = c(1,3))
pr_train = prediction(as.numeric(pred_train),train$y)
prf_train = performance(pr_train, measure = "tpr", x.measure = "fpr")
plot(prf_train)

pr_test1 = prediction(as.numeric(pred_test1),test1$y)
prf_test1 = performance(pr_test1, measure = "tpr",x.measure = "fpr")
plot(prf_test1)

pr_test2 = prediction(as.numeric(pred_test2),test2$y)
prf_test2 = performance(pr_test2, measure = "tpr",x.measure = "fpr")
plot(prf_test2)

auc_train = performance(pr_train, measure = "auc")
auc_test1 = performance(pr_test1, measure = "auc")
auc_test2 = performance(pr_test2, measure = "auc")


```


```{r}


metrics = cbind(train_cm$byClass, test1_cm$byClass,test2_cm$byClass)
acc = cbind(train_cm$overall["Accuracy"],test1_cm$overall["Accuracy"], test2_cm$overall["Accuracy"])
auc = cbind(auc_train@y.values,auc_test1@y.values,auc_test2@y.values)
rownames(auc) = c("Area Under Curve")
result = rbind(acc,auc,metrics)

colnames(result) = c("Train","Test1","Test2")
kable(result)
```



####  model using all predictors
####  Attribute selection using backward/forward/subset selection
####  Calculate missclassification Rate (confusion matrix)
####  Plot ROC curve (TPR vs FPR), AIC plot, RMSE, adjr2
####  Pick best using AIC,BIC and anova Test
####  Report Accuracy



## Results

### Compare Models (Test and Train Accuracy)



## Discussion

- Precision: What percentage of tuples labeled as positively is actually positive?

- Recall: What percentage of positve tuples are labeled as positive?

- AUC: The area under the curve is a measure of the accuracy of the model

- Misclassification rate
- Thresold
- Select best among 3 and why?
## Appendix

UCI - citation
